<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Hao Hu</title>
  <meta name="description" content="Hao Hu's homepage">

  <link href="./css/github-light.css" rel="stylesheet">
  <link href="./css/style.css" rel="stylesheet">
  <style> 
    .visit{text-align:center} 
    </style> 
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BV2KEK1S65"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BV2KEK1S65');
</script>

<body>
  <div class="container">
    <header>
      <h1>Hao Hu</h1>
      <img src="./assets/huhao.jpg" style="width:230px;"><br />
      <p>
        <br/>
        Ph.D. Student in Computer Science <br />
        IIIS, Tsinghua University <br />
        Beijing, China
        <br/>
        <a href="mailto: huh22@mails.tsinghua.edu.cn"> <img src="./assets/email_icon.png" alt="Email" style="width:19px;border:0;"> Email </a> <br/> 
        <a href="https://github.com/mousehu"> <img src="./assets/github.png" alt=" Github" style="width:16px;height:15px;border:0;"> Github </a> <br/>
        <a href="https://www.linkedin.com/in/hao-hu-tsinghua/"> <img src="./assets/linkedin3.png" alt=" Linkedin" style="width:16px;height:15px;border:0;"> Linkedin </a> <br/>
        <a href="./assets/resume.pdf"> <img src="./assets/cv_icon.png" alt=" CV" style="width:14px;height:14px;border:0;"> CV </a><br />
        <a href="https://scholar.google.com/citations?user=mhDH3VYAAAAJ&hl=en"> <img src="./assets/ggscholar_black.png" alt="gscholar" style="width:14px;height:14px;border:0;"> Google Scholar </a><br /><br/>
    </header>

    <section class="content">
      <h3>About</h3>
      <p>
  			I am currently a Ph.D. student working with <a href="http://people.iiis.tsinghua.edu.cn/~zhang/"> Prof. Chongjie Zhang </a> and <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/"> Prof. Yang Gao</a> at <a href="https://iiis.tsinghua.edu.cn/en/"> 
        Institute for Interdisciplinary Information Sciences</a>, Tsinghua University, headed by <a href="https://iiis.tsinghua.edu.cn/yao/"> Prof. Andrew Yao</a>. My primary research 
        goal is to develop innovative models in a principled approach to enable sample-efficient and generalizable reinforcement learning.
        My research interests include offline reinforcement learning, reinforcement learning with large language models and reinforcement learning theory.
        
      </p>
      <p>
        Please feel free to contact me if you have interests in collaborating with me.
      </p>
      <h3>Publications and Preprints</h3>
      <ul class="sparse-list">
         <!-- <li>
          <strong>Tonghan Wang</strong>, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang <br />
          <b style=color:#66ccff;">RODE: Learning Roles to Decompose Multi-Agent Tasks.</b> <br />
          <b>ICLR 2021: </b> International Conference on Learning Representations <br/>
          <a href="https://arxiv.org/abs/2010.01523"> PDF </a> | <a href="https://sites.google.com/view/rode-marl"> Video </a> | <a href="https://github.com/TonghanWang/RODE"> Code </a>
        </li> -->
        <li>
          <b style=color:#0101ad;">Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents </b> <br />
          Zhihan Liu*, <strong>Hao Hu*</strong>, Shenao Zhang*, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang <br />
          <i>Arxiv Preprint</i> <br/>
          <a href="https://arxiv.org/pdf/2309.17382.pdf"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;"> One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploratio</b> <br />
          Zhihan Liu* Miao Lu* Wei Xiong* Han Zhong, <strong>Hao Hu</strong>, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran
          Wang<br />
          <i>Thirty-seventh Conference on Neural Information Processing Systems <b>(NeurIPS <b style="color:#A020F0;">Spotlight</b>), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2305.18258.pdf"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">  Unsupervised Behavior Extraction via Random Intent Priors </b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang* Jianing Ye, Ziqing Mai, Chongjie Zhang <br />
          <i>Thirty-seventh Conference on Neural Information Processing Systems <b>(NeurIPS), 2023</b></i> <br/>
          <a title="Coming soon"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?</b> <br />
          Rui Yang, Yong Lin, Xiaoteng Ma, <strong>Hao Hu</strong> , Chongjie Zhang, Tong Zhang <br />
          <i>Eleventh International Conference on Learning Representations <b>(ICLR), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2305.18882.pdf"> PDF </a> | <a href="https://github.com/YangRui2015/GOAT"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">The Provable Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning</b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Qianchuan Zhao, Chongjie Zhang <br />
          <i>Eleventh International Conference on Learning Representations <b>(ICLR), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2302.13493.pdf"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery</b> <br />
          Yiqin Yang*, <strong>Hao Hu*</strong>, Wenzhe Li*, Siyuan Li, Jun Yang, Qianchuan Zhao, Chongjie Zhang <br />
          <i>Proceedings of the AAAI Conference on Artificial Intelligence <b>(AAAI), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2212.01105.pdf"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">On the Role of Discount Factor in Offline Reinforcement Learning</b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Qianchuan Zhao, Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2022</b></i> <br/>
          <a href="https://arxiv.org/pdf/2206.03383.pdf"> PDF </a>
        </li>

        <li>
          <b style=color:#0101ad;">Offline Reinforcement Learning with Value-based Episodic Memory</b> <br />
          <strong>Hao Hu*</strong>, Xiaoteng Ma*, Yiqin Yang*, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, Bin Liang <br />
          <i>Tenth International Conference on Learning Representations <b>(ICLR), 2022</b></i> <br/>
          <a href="https://arxiv.org/pdf/2110.09796.pdf"> PDF </a> | <a href="https://github.com/YiqinYang/VEM"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">On the Estimation Bias in Double Q-Learning</b> <br />
          Zhizhou Ren, Guangxiang Zhu, <strong>Hao Hu</strong>, Beining Han, Jianglun Chen, Chongjie Zhang <br />
          <i>Thirty-fifth Conference on Neural Information Processing Systems <b>(NeurIPS), 2021</b></i> <br/>
          <a href="https://arxiv.org/pdf/2109.14419.pdf"> PDF </a> | <a href="https://github.com/Stilwell-Git/Doubly-Bounded-Q-Learning"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Generalizable Episodic Memory for Deep Reinforcement Learning </b> <br />
          <strong>Hao Hu</strong>, Jianing Ye, Zhizhou Ren, Guangxiang Zhu, and Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2021</b></i><br/>
          <a href="https://arxiv.org/abs/2103.06469"> PDF </a> | <a href="https://github.com/MouseHu/GEM"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration</b> <br />
          Jin Zhang*, Jianhao Wang*, <strong>Hao Hu</strong>, Tong Chen, Yingfeng Chen, Changjie Fan, Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2021</b></i><br/>
          <a href="https://arxiv.org/abs/2006.08170"> PDF </a> | <a href="https://github.com/NagisaZj/MetaCURE-Public"> Code </a>
        </li>

      </ul>

                                                                                                                          
      <h3> Experience </h3>
      <ul>
        
        <li><b> Engineering Intern (Jun. 2018 -- Sept. 2018)</b> <br />
          News & Relevance Team, STCA, Microsoft<br />
          
        </li>
        <li><b> Visiting Scholar  (Mar. 2023 -- Now)</b> <br />
          Northwestern University, Supervisor: <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a> <br />
          
        </li>

        <li><b> Teaching Assistant </b> <br />
          Artificial Intelligence: Principles and Techniques, Fall, 2020<br />
          Artificial Intelligence: Principles and Techniques, Fall, 2021<br />
          Deep Reinforcement Learning, Spring, 2022 <br /></li>
      </ul>

      <h3>Services</h3>
      <ul>
        <li>Reviewer of AAMAS 2023</li>
      </ul>

      <h3>Education</h3>
      <ul>
        <li>
          <b> Ph.D. Student in Computer Science </b> <br/>
          IIIS, Tsinghua University @ Beijing, China, 2019 -- Present <br/>
          Deep Reinforcement Learning
        </li>                                                                                                                  
        <li>
          <b> B.Sc. in Theoretical and Applied Mechanics </b> <br/>
          Peking University @ Beijing, China, 2015 -- 2019 <br/>
          Double major: Computer Science and Technology<br/>
          Thesis: The Convergence of Momentum Policy Prediction Method
          and Its Application in Multi-agent Reinforcement Learning
        </li>
      </ul>
      <!-- <h3>On-going Projects</h3>
      <ul>
        <li>
          <b> Episodic Control with Skills</b> <br/>
          Use episodic memory for skill storage and retrival, enabling efficient exploration and learning with skills <br/>
        </li>                                                                                                                  
        <li>
          <b> Zero-shot Reinforcement Learning with Semantic World Models </b> <br/>
          Use semantic world model to enable zero-shot transfer and combinatorial generalization of skills to new tasks <br/>
        </li>
       <li>
          <b> Lightning RL: A light-weight, auto-tuning and distributed RL framework </b> <br/>
          See <a href="https://github.com/MouseHu/RLLightning"> Github Link </a>  <br/>
        </li> -->
      <!-- </ul> --> 
      <!-- <div class="visit">
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=hNvBXX4UmAGYx0olALmdhFqEyvFSp289uxN0uJasOqk"></script>
      </div> -->
      <!-- \section{On-going Projects}
\begin{itemize}
    \item Episodic Skill Control\par
    \textit{blah blah}
    \item Zero-shot Reinforcement Learning with senmantic world models\par
    \textit{blah blah}
    \item Lightning RL: A light-weight, auto-tuning and distributed RL framework\par
    \textit{blah blah}
\end{itemize} -->
    </section>
  </div>
</body>
</html>
