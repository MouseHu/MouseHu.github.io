<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Hao Hu</title>
  <meta name="description" content="Hao Hu's homepage">

  <link href="./css/github-light.css" rel="stylesheet">
  <link href="./css/style.css" rel="stylesheet">
  <style> 
    .visit{text-align:center} 
    </style> 
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-BV2KEK1S65"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-BV2KEK1S65');
</script>

<body>
  <div class="container">
    <header>
      <h1>Hao Hu</h1>
      <img src="./assets/huhao.jpg" style="width:230px;"><br />
      <p>
        <br/>
        Ph.D. Candidate in Computer Science <br />
        IIIS, Tsinghua University <br />
        Beijing, China
        <br/>
        <a href="mailto: huh22@mails.tsinghua.edu.cn"> <img src="./assets/email_icon.png" alt="Email" style="width:19px;border:0;"> Email </a> <br/> 
        <a href="https://github.com/mousehu"> <img src="./assets/github.png" alt=" Github" style="width:16px;height:15px;border:0;"> Github </a> <br/>
        <a href="https://www.linkedin.com/in/hao-hu-tsinghua/"> <img src="./assets/linkedin3.png" alt=" Linkedin" style="width:16px;height:15px;border:0;"> Linkedin </a> <br/>
        <img src="./assets/cv_icon.png" alt=" CV" style="width:14px;height:14px;border:0;"> <a href="./assets/resume.pdf">  CV </a> / <a href="./assets/resume_chinese.pdf" style="font-weight:100;"> 中文简历 </a><br />
        <a href="https://scholar.google.com/citations?user=mhDH3VYAAAAJ&hl=en"> <img src="./assets/ggscholar_black.png" alt="gscholar" style="width:14px;height:14px;border:0;"> Google Scholar </a><br /><br/>
    </header>

    <section class="content">
      <h3>About</h3>
      <p>
  			I am currently a Ph.D. candidate working with <a href="http://people.iiis.tsinghua.edu.cn/~zhang/"> Prof. Chongjie Zhang </a> and <a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/"> Prof. Yang Gao</a> at <a href="https://iiis.tsinghua.edu.cn/en/"> 
        Institute for Interdisciplinary Information Sciences</a>, Tsinghua University, headed by <a href="https://iiis.tsinghua.edu.cn/yao/"> Prof. Andrew Yao</a>. I received my B.Sc. degree in Theoretical and Applied Mechanics from Peking University in 2019, with a double major in Computer Science. During my Ph.D., I was fortunate to visit Northwestern University and work with <a href="https://zhaoranwang.github.io/"> Prof. Zhaoran Wang </a>. 
      </p>
      <p>
        My primary research goal is to build intelligent and autonomous agents and deepen the understanding of our own intelligence. Toward this goal, my research interest has been focused on designing algorithms for decision-making in a principled and data-driven approach, which are essential for agents to act and learn effectively in complex environments. Current directions I am working on include offline reinforcement learning, reinforcement learning with foundation models, and reinforcement learning theory.

      </p>
      <p>
        Please feel free to contact me if you are interested in collaborating with me.
      </p>
      
      <h3>Publications and Preprints</h3>
      <ol class="sparse-list">
         <!-- <li>
          <strong>Tonghan Wang</strong>, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and Chongjie Zhang <br />
          <b style=color:#66ccff;">RODE: Learning Roles to Decompose Multi-Agent Tasks.</b> <br />
          <b>ICLR 2021: </b> International Conference on Learning Representations <br/>
          <a href="https://arxiv.org/abs/2010.01523"> PDF </a> | <a href="https://sites.google.com/view/rode-marl"> Video </a> | <a href="https://github.com/TonghanWang/RODE"> Code </a>
        </li> -->

        <li>
          <b style=color:#0101ad;">Bayesian Design Principles for Offline-to-Online Reinforcement Learning </b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Jianing Ye, Chengjie Wu, Ziqing Mai, Yujing Hu, Tangjie Lv, Changjie Fan, Qianchuan Zhao, Chongjie Zhang <br />
          <i>Forty-first International Conference on Machine Learning <b>(ICML), 2024</b></i> <br/>
          <a title="Coming soon"> PDF </a> | <a title="Coming soon"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Reason for Future, Act for Now: A Principled Architecture for Autonomous LLM Agents </b> <br />
          Zhihan Liu*, <strong>Hao Hu*</strong>, Shenao Zhang*, Hongyi Guo, Shuqi Ke, Boyi Liu, Zhaoran Wang <br />
          <i>Forty-first International Conference on Machine Learning <b>(ICML), 2024</b></i> <br/>
          <i>NeurIPS Workshop on Foundation Models for Decision Making, 2023</i> <br/>
          <a href="https://arxiv.org/pdf/2309.17382.pdf"> PDF </a> | <a href="https://github.com/agentification/RAFA_code"> Code </a> | <a href="https://agentification.github.io/RAFA/"> Project Page </a>
        </li>
        <li>
          <b style=color:#0101ad;">Planning, Fast and Slow: Online Reinforcement Learning with Action-Free Offline Data via Multiscale Planners </b> <br />
          Chengjie Wu*, <strong>Hao Hu*</strong>, Yiqin Yang, Ning Zhang, Chongjie Zhang <br />
          <i>Forty-first International Conference on Machine Learning <b>(ICML), 2024</b></i> <br/>
          <a title="Coming soon"> PDF </a> | <a title="Coming soon"> Code </a>

        </li>
        <li>
          <b style=color:#0101ad;"> One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration</b> <br />
          Zhihan Liu* Miao Lu* Wei Xiong* Han Zhong, <strong>Hao Hu</strong>, Shenao Zhang, Sirui Zheng, Zhuoran Yang, Zhaoran
          Wang<br />
          <i>Thirty-seventh Conference on Neural Information Processing Systems <b>(NeurIPS <b>Spotlight</b>), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2305.18258.pdf"> PDF </a> | <a href="https://github.com/agentification/MEX"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">  Unsupervised Behavior Extraction via Random Intent Priors </b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang* Jianing Ye, Ziqing Mai, Chongjie Zhang <br />
          <i>Thirty-seventh Conference on Neural Information Processing Systems <b>(NeurIPS), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2310.18687.pdf"> PDF </a> | <a href="https://github.com/MouseHu/UBER"> Code </a>
          <!-- <a title="Coming soon"> PDF </a> | <a title="Coming soon"> Code </a> -->
        </li>
        <li>
          <b style=color:#0101ad;">What is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?</b> <br />
          Rui Yang, Yong Lin, Xiaoteng Ma, <strong>Hao Hu</strong> , Chongjie Zhang, Tong Zhang <br />
          <i>Eleventh International Conference on Learning Representations <b>(ICLR), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2305.18882.pdf"> PDF </a> | <a href="https://github.com/YangRui2015/GOAT"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">The Provable Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning</b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Qianchuan Zhao, Chongjie Zhang <br />
          <i>Eleventh International Conference on Learning Representations <b>(ICLR), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2302.13493.pdf"> PDF </a> | <a href="https://github.com/YiqinYang/PDS"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery</b> <br />
          Yiqin Yang*, <strong>Hao Hu*</strong>, Wenzhe Li*, Siyuan Li, Jun Yang, Qianchuan Zhao, Chongjie Zhang <br />
          <i>Proceedings of the AAAI Conference on Artificial Intelligence <b>(AAAI), 2023</b></i> <br/>
          <a href="https://arxiv.org/pdf/2212.01105.pdf"> PDF </a> | <a href="https://github.com/YiqinYang/LPD"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">On the Role of Discount Factor in Offline Reinforcement Learning</b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Qianchuan Zhao, Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2022</b></i> <br/>
          <a href="https://arxiv.org/pdf/2206.03383.pdf"> PDF </a> | <a href="https://github.com/YiqinYang/Offline-Gamma"> Code </a>
        </li>

        <li>
          <b style=color:#0101ad;">Offline Reinforcement Learning with Value-based Episodic Memory</b> <br />
          Xiaoteng Ma*, Yiqin Yang*, <strong>Hao Hu*</strong>, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan Zhao, Bin Liang <br />
          <i>Tenth International Conference on Learning Representations <b>(ICLR), 2022</b></i> <br/>
          <a href="https://arxiv.org/pdf/2110.09796.pdf"> PDF </a> | <a href="https://github.com/YiqinYang/VEM"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">On the Estimation Bias in Double Q-Learning</b> <br />
          Zhizhou Ren, Guangxiang Zhu, <strong>Hao Hu</strong>, Beining Han, Jianglun Chen, Chongjie Zhang <br />
          <i>Thirty-fifth Conference on Neural Information Processing Systems <b>(NeurIPS), 2021</b></i> <br/>
          <a href="https://arxiv.org/pdf/2109.14419.pdf"> PDF </a> | <a href="https://github.com/Stilwell-Git/Doubly-Bounded-Q-Learning"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Generalizable Episodic Memory for Deep Reinforcement Learning </b> <br />
          <strong>Hao Hu</strong>, Jianing Ye, Zhizhou Ren, Guangxiang Zhu, and Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2021</b></i><br/>
          <a href="https://arxiv.org/abs/2103.06469"> PDF </a> | <a href="https://github.com/MouseHu/GEM"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">MetaCURE: Meta Reinforcement Learning with Empowerment-Driven Exploration</b> <br />
          Jin Zhang*, Jianhao Wang*, <strong>Hao Hu</strong>, Tong Chen, Yingfeng Chen, Changjie Fan, Chongjie Zhang <br />
          <i>International Conference on Machine Learning <b>(ICML), 2021</b></i><br/>
          <a href="https://arxiv.org/abs/2006.08170"> PDF </a> | <a href="https://github.com/NagisaZj/MetaCURE-Public"> Code </a>
        </li>
        <li>
          <b style=color:#0101ad;">Query-Efficient Offline Preference-Based Reinforcement Learning via In-Dataset Exploration</b> <br />
          <strong>Hao Hu*</strong>, Yiqin Yang*, Shuai Wang, Bo Liu, Yang Gao, Chongjie Zhang <br />
          <i>Under Review</i><br/>
          <a href="https://openreview.net/pdf?id=GOvTGntFNj"> PDF </a>
        </li>
        <li>
          <b style=color:#0101ad;">Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets</b> <br />
          Yihuan Mao, Chengjie Wu, Xi Chen, <strong>Hao Hu</strong>, Ji Jiang, Tianze Zhou, Tangjie Lv, Changjie Fan, Zhipeng Hu, Yi Wu, Yujing Hu, Chongjie Zhang <br />
          <i>Under Review</i><br/>
          <a href="https://openreview.net/pdf?id=rnHNDihrIT"> PDF </a>
        </li>
      </ol>

      <h3> Experience </h3>
      <ul>
        <li><b> Visiting Scholar  (Mar. 2023 -- Sept. 2023)</b> <br />
          NURL group, IEMS, Northwestern University<br />
          Supervisor: <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a> <br />
        </li>
        <li><b> Engineering Intern (Jun. 2018 -- Sept. 2018)</b> <br />
          News & Relevance Team, STCA, Microsoft<br />
        </li>
        <li><b> Research Intern  (Nov. 2017 -- Jun. 2018)</b> <br />
          VMC group, Peking University<br />
          Supervisor: <a href="https://pkuvmc.com/">Shiliang Zhang</a> <br />
        </li>
      </ul> 

      <h3> Services </h3>
      <ul>
        <li><b> Teaching Assistant </b> <br />
          Artificial Intelligence: Principles and Techniques, Fall, 2020<br />
          Artificial Intelligence: Principles and Techniques, Fall, 2021<br />
          Deep Reinforcement Learning, Spring, 2022 <br /></li>
          <li><b> Student Instructor  </b> <br />
            Theoretical Mechanics, Spring, 2018<br />
          <li><b> Reviewer </b> <br />
            AAMAS 2023, NeurIPS 2023, ICLR 2024<br />
      </ul>

      <h3>Selected Talks</h3>
      <ul>
        <li><b>Data-driven Reinforcement Learning</b><br />
          Bytedance AI Lab, 2023.11 | <a href="./assets/Data-Driven RL.pdf">PDF</a> <br />
          
        </li>

        <li><b>Unsupervised Behavior Extraction via Random Intent Priors</b><br />
          RL China, 2023.10 | <a href="./assets/uber_rlchina.pdf">PDF</a> <br />
        </li>
        <li><b>On the Role of Discount Factor in Offline
          Reinforcement Learning</b><br />
          RL China, 2022.06 | <a href="./assets/gamma_rlchina.pdf">PDF</a> | <a href="https://www.bilibili.com/video/BV1uf4y1Z7ED">Video</a> <br />
        </li>
        
        <li><b>Generalizable Episodic Memory for Deep Reinforcement Learning</b><br />
          Nanjing University, 2021.07 | <a href="./assets/GEM_Nanjing.pdf">PDF</a> <br />
        </li>
      </ul>
      <!-- <iframe src="//player.bilibili.com/player.html?aid=300536802&bvid=BV1uf4y1Z7ED&cid=762762126&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe> -->

      <h3>Education</h3>
      <ul>
        <li>
          <b> Ph.D. Student in Computer Science </b> <br/>
          IIIS, Tsinghua University @ Beijing, China, 2019 -- Present <br/>
          Deep Reinforcement Learning
        </li>                                                                                                                  
        <li>
          <b> B.Sc. in Theoretical and Applied Mechanics </b> <br/>
          Peking University @ Beijing, China, 2015 -- 2019 <br/>
          Double major: Computer Science and Technology<br/>
          Thesis: The Convergence of Momentum Policy Prediction Method
          and Its Application in Multi-agent Reinforcement Learning
        </li>
      </ul>
      <!-- <h3>On-going Projects</h3>
      <ul>
        <li>
          <b> Episodic Control with Skills</b> <br/>
          Use episodic memory for skill storage and retrival, enabling efficient exploration and learning with skills <br/>
        </li>                                                                                                                  
        <li>
          <b> Zero-shot Reinforcement Learning with Semantic World Models </b> <br/>
          Use semantic world model to enable zero-shot transfer and combinatorial generalization of skills to new tasks <br/>
        </li>
       <li>
          <b> Lightning RL: A light-weight, auto-tuning and distributed RL framework </b> <br/>
          See <a href="https://github.com/MouseHu/RLLightning"> Github Link </a>  <br/>
        </li> -->
      <!-- </ul> --> 
      <!-- <div class="visit">
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=hNvBXX4UmAGYx0olALmdhFqEyvFSp289uxN0uJasOqk"></script>
      </div> -->
      <!-- \section{On-going Projects}
\begin{itemize}
    \item Episodic Skill Control\par
    \textit{blah blah}
    \item Zero-shot Reinforcement Learning with senmantic world models\par
    \textit{blah blah}
    \item Lightning RL: A light-weight, auto-tuning and distributed RL framework\par
    \textit{blah blah}
\end{itemize} -->
    </section>
    <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=fTCu_cQksIUNiuvXAlgZR1B_GiZTgfjOgPQiQI-_p0A&cl=ffffff&w=a"></script> -->
  </div>
</body>
</html>
